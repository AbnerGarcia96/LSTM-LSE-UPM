{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ed38f-60f4-4aba-bc8f-3ec3c5a49efc",
   "metadata": {},
   "source": [
    "# 1. Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e92d54-7177-4c71-ae36-e9dfc724e261",
   "metadata": {},
   "source": [
    "**Nota**: En caso de necesitar instalar las librerías se puede ejecutar el siguiente bloque antes de realizar las importaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f16b5-72c3-4299-ac03-8628b8e8449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.10.0.82 numpy==1.23.5 mediapipe==0.10.14 tensorflow==2.12.0rc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa405f8a-1b4e-4718-b81c-ae2b7ee33a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Para la partición de datos de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Para la creación del modelo con LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Para la evaluación del modelo con LSTM\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555f88a-1628-4170-a3f8-af97f0f9b0a8",
   "metadata": {},
   "source": [
    "# 2. Uso de OpenCV para detectar partes del cuerpo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329a082-efbc-4c63-bb15-f20e78ccea75",
   "metadata": {},
   "source": [
    "## 2.1. Detección de puntos de interés (landmarks) de cara, brazos y manos con OpenCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e02fa2-8edb-4292-a350-e2cf467b7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Se convierte porque CV2 trabaja con BGR y el modelo con RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Se regresa al valor por defecto BGR de CV2\n",
    "\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2aad48-5b8d-4bbc-8b6d-7846d3c6dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_draw.draw_landmarks(\n",
    "        image, \n",
    "        results.face_landmarks, \n",
    "        mp_hol.FACEMESH_TESSELATION,\n",
    "        mp_draw.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=1),\n",
    "        mp_draw.DrawingSpec(color=(255,255,255), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    mp_draw.draw_landmarks(\n",
    "        image, \n",
    "        results.pose_landmarks, \n",
    "        mp_hol.POSE_CONNECTIONS,\n",
    "        mp_draw.DrawingSpec(color=(255,0,0), thickness=1, circle_radius=1),\n",
    "        mp_draw.DrawingSpec(color=(255,0,0), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    mp_draw.draw_landmarks(\n",
    "        image, \n",
    "        results.left_hand_landmarks, \n",
    "        mp_hol.HAND_CONNECTIONS,\n",
    "        mp_draw.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "        mp_draw.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1)\n",
    "    )\n",
    "    mp_draw.draw_landmarks(\n",
    "        image, \n",
    "        results.right_hand_landmarks, \n",
    "        mp_hol.HAND_CONNECTIONS,\n",
    "        mp_draw.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "        mp_draw.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27665624-abfe-4a85-8d35-80abe17831df",
   "metadata": {},
   "source": [
    "## 2.2. Extracción de puntos de interés (landmarks) a un arreglo de numpy para ser utilizado por la LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f02e6-6840-4e92-88e9-1ddfdf228dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks_array(results):\n",
    "    face = np.array([[r.x, r.y, r.z] for r in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*4) \n",
    "    pose = np.array([[r.x, r.y, r.z, r.visibility] for r in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4) \n",
    "    left_hand = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hand = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([face, pose, left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2618862-3512-466e-b9f5-4162934eb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hol = mp.solutions.holistic\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "holistic = mp_hol.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45f011-801c-46bd-a4bc-8ad3332359d6",
   "metadata": {},
   "source": [
    "**Nota**: Este bloque de código es para verificar que los landmarks se están mostrando correctamente en la pantalla, se puede omitir en la ejecución del programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669a5a1-1d8d-4786-b773-941cab709e6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    # Obtiene la imagen de la camara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Realiza la detección de rostro, brazos y manos con mediapipe\n",
    "    image, results = body_detection(frame, holistic)\n",
    "    \n",
    "    # Dibuja los puntos detectados con el modelo holistico y los agrega a la captura de video\n",
    "    draw_landmarks(image, results)\n",
    "    \n",
    "    # Muestra captura de video en pantalla\n",
    "    cv2.imshow('OpenCV', image)\n",
    "\n",
    "    # En caso de oprimir ESC, sale del While\n",
    "    if cv2.waitKey(27) & 0xFF == ord('\\x1b'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7af83c-eab2-46bd-8dd0-a0ff06249181",
   "metadata": {},
   "source": [
    "# 3. Generación de conjunto de datos para el entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa6a25-86f9-406c-90f4-205930b34d17",
   "metadata": {},
   "source": [
    "**Nota**: las secciones 3.2. y 3.3. pueden omitirse si ya se generaron datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60400b7-3661-4906-a1f4-597838607156",
   "metadata": {},
   "source": [
    "## 3.1. Configuración de la carpeta de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac15d0-768b-49a6-8fb7-45b4b3991fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de open CV y MediaPipe para obtener los landmarks\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hol = mp.solutions.holistic\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "holistic = mp_hol.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Ruta de la carpeta de datos\n",
    "folder_path = os.path.join('data')\n",
    "\n",
    "# Configuracion del numero de secuencias a generar por cada gesto y la cantidad de frames a recolectar por cada secuencia\n",
    "gestures = np.array(['hola','gracias'])\n",
    "seq_amount = 30 # 30 secuencias por gesto\n",
    "seq_length = 30 # 30 frames por secuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075127a-a126-49f5-a9fd-c64c8b873bb7",
   "metadata": {},
   "source": [
    "## 3.2. Creación de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4597ad0-bbea-42b8-b658-65996b8138d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for g in gestures:\n",
    "    for s in range(seq_amount):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(folder_path, g, str(s)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299da78-933b-4cd8-8de7-c92304352109",
   "metadata": {},
   "source": [
    "## 3.3. Programa para la captación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaed9dc-7677-412a-bfe0-2fa4534b169b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for g in gestures:\n",
    "    for s in range(seq_amount):\n",
    "        for frame_number in range(seq_length):\n",
    "            \n",
    "            # Obtiene la imagen de la camara\n",
    "            ret, frame = cap.read()\n",
    "        \n",
    "            # Realiza la detección de rostro, brazos y manos con mediapipe\n",
    "            image, results = body_detection(frame, holistic)\n",
    "            \n",
    "            # Dibuja los puntos detectados con el modelo holistico y los agrega a la captura de video\n",
    "            draw_landmarks(image, results)\n",
    "\n",
    "            # Inicia la recolección de datos de los gestos, muestra en pantalla mensaje para hacer gestos y espera 1 segundo para tomar nuevos datos\n",
    "            if frame_number == 0:\n",
    "                cv2.putText(image, 'Starting data gathering', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, f'Gathering frames for {g} - Video No. {s}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "                cv2.waitKey(1000)\n",
    "            else:\n",
    "                cv2.putText(image, f'Gathering frames for {g} - Video No. {s}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Guarda los datos obtenidos en un archivo\n",
    "            keypoints = get_landmarks_array(results)\n",
    "            output_path = os.path.join(folder_path, g, str(s), str(frame_number))\n",
    "            np.save(output_path, keypoints)\n",
    "            \n",
    "            # Muestra captura de video en pantalla\n",
    "            cv2.imshow('OpenCV', image)\n",
    "        \n",
    "            # En caso de oprimir ESC, sale del While\n",
    "            if cv2.waitKey(27) & 0xFF == ord('\\x1b'):\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ff168-2d74-4605-9d28-9bf47b350ea9",
   "metadata": {},
   "source": [
    "# 4. Preprocesamiento y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38843d7c-ca98-4d6f-9d78-52376e7cab5e",
   "metadata": {},
   "source": [
    "## 4.1. Obtención de los datos generados en la sección 3. Generación de conjunto de datos para el entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48494b06-b67b-4e86-9512-07972d968402",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(gestures)}\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for gesture in gestures:\n",
    "    for sequence in range(seq_amount):\n",
    "        window = []\n",
    "        for frame_number in range(seq_length):\n",
    "            res = np.load(os.path.join(folder_path, gesture, str(sequence), f\"{frame_number}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[gesture])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652811ac-a773-41a0-b904-883df5e11483",
   "metadata": {},
   "source": [
    "## 4.2. Partición del conjunto de datos en datos de entrenamiento y datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8313c60-a9fb-45a4-a57f-4fa5064c1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1d507-d6ff-4fbd-abb5-07eb60adec42",
   "metadata": {},
   "source": [
    "# 5. Construcción del modelo con LSTM para la predicción de gestos en lengua de signos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b686a-cf7d-47f4-a153-29243eba3958",
   "metadata": {},
   "source": [
    "## 5.1. Definición de los parámetros para guardar los logs en TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06521e0-449f-49ca-b2e9-cfba9daf06af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logs_path = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22364ad-5add-4646-98a1-b6bcfd97026f",
   "metadata": {},
   "source": [
    "## 5.2. Configuración del modelo con LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c72c1-e50c-49f0-9911-8fc1234ac75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(gestures.shape[0], activation='softmax'))\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36208f-77a5-4956-8991-098a2cfe7be9",
   "metadata": {},
   "source": [
    "## 5.3. Entrenamiento del modelo de predicción de gestos en lengua de signos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6af8a-9781-4a53-8c7e-61fe771a8073",
   "metadata": {},
   "source": [
    "**Nota**: En caso de tener un modelo ya entrenado, se puede omitir esta parte y pasar a la sección 5.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c65ac-5670-44f3-b31c-cc07243a1494",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "model.summary()\n",
    "model.save('predictor_LSE.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa48966-fea4-4ba1-a52d-525ee1ae7f6b",
   "metadata": {},
   "source": [
    "## 5.4. Carga del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e9639-18f4-4776-be4b-72dbc3139ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('predictor_LSE.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b6692-5816-4eb2-a5f9-e0fde5da0a32",
   "metadata": {},
   "source": [
    "## 5.5. Evaluación del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e69e92-3498-4e74-a11b-b8e44ee3a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test)\n",
    "pred_test_real = np.argmax(y_test, axis=1).tolist()\n",
    "pred_test = np.argmax(pred_test, axis=1).tolist()\n",
    "print(\"Matriz de confusión: \\n\", multilabel_confusion_matrix(pred_test_real, pred_test))\n",
    "print(\"\\n Precisión: \", accuracy_score(pred_test_real, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c38a7-2808-4964-a2bf-01fa913fc494",
   "metadata": {},
   "source": [
    "# 6. Pruebas de reconocimiento en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8204277-bd2d-4383-b625-07ae934dd138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.6\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Obtiene la imagen de la camara\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Realiza la detección de rostro, brazos y manos con mediapipe\n",
    "    image, results = body_detection(frame, holistic)\n",
    "    \n",
    "    # Dibuja los puntos detectados con el modelo holistico y los agrega a la captura de video\n",
    "    draw_landmarks(image, results)\n",
    "\n",
    "    # Procesamiento de predicción\n",
    "    landmarks = get_landmarks_array(results)\n",
    "    sequence.append(landmarks)\n",
    "    sequence = sequence[-30:]\n",
    "    pred = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "    predictions.append(np.argmax(pred))\n",
    "\n",
    "    if len(sequence) == 30:\n",
    "        pred = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        print(f'Prediction = {gestures[np.argmax(pred)]}, {pred}')\n",
    "        predictions.append(np.argmax(pred))\n",
    "\n",
    "    # Visualización de resultados\n",
    "    if np.unique(predictions[-10:])[0]== np.argmax(pred):\n",
    "        if pred[np.argmax(pred)] > threshold:\n",
    "            if len(sentence) > 0:\n",
    "                if gestures[np.argmax(pred)] != sentence[-1]:\n",
    "                    sentence.append(gestures[np.argmax(pred)])\n",
    "            else:\n",
    "                sentence.append(gestures[np.argmax(pred)])\n",
    "    \n",
    "    if len(sentence) > 5:\n",
    "        sentence = sentence[-5:]\n",
    "\n",
    "    output_text = ' '.join(sentence)\n",
    "    cv2.rectangle(image, (0,0), (640,40), (245,117,16), -1)\n",
    "    cv2.putText(image, output_text, (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    # Muestra captura de video en pantalla\n",
    "    cv2.imshow('OpenCV', image)\n",
    "\n",
    "    # En caso de oprimir ESC, sale del While\n",
    "    if cv2.waitKey(27) & 0xFF == ord('\\x1b'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
